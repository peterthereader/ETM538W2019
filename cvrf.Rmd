---
title: "Blood Donation Classification Prediction"
author: "Andey Nunes"
date: "`r format(Sys.time(), '%B %d, %Y')`"
output:
  pdf_document: default
  html_document:
    df_print: paged
subtitle: using Cross Validation with Random Forest
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(echo = TRUE)

# BEFORE KNITTING THIS DOCUMENT, ENSURE THAT THE FOLLOWING PACKAGES
# ARE INSTALLED & UPDATED 

packages <- c("corrplot", "knitr", "Metrics", "ranger", "rsample", "tidyverse")

lapply(packages, require, character.only = T)

options(digits = 4, scipen = 999)

set.seed(123)
```

# Data Description
Our group is entered into a competitiion for a classification problem:
https://www.drivendata.org/competitions/2/warm-up-predict-blood-donations/

> The business question is: using data about student blood donations, can we predict if a student will donate blood next time? 

The competition specifies the training and test files, so we do not need to set aside test data. First we load and inspect the training set for structure, variable types, and pairwise correlations.

```{r load data}
training <- read_csv("projectdata.csv")
glimpse(training)
```

The variable `X1` has `r length(unique(training$X1))` unique entries and is therefore likely to be a student or patient ID number. We can ignore it for modeling purposes. I'm also going to shorten the variable names as follows:

+ `Months since Last Donation` is renamed `recency` to indicate how many months ago the last donation event was
+ `Months since First Donation` is renamed `time` to indicate the total time span in months since the first donation event
+ `Number of Donations` is renamed `frequency` 
+ `Total Volume Donated (c.c.)` is renamed `volume` the original data set library makes note that this field indicates the monetary measure being used for the business case
+ `Made Donation in March 2007` is renamed `target` and is the binary variable that we are trying to predict/classify
 
** Correlation Matrix of Blood Donation Data **

```{r correlations}
training <- training[,-1]

names(training) <- c("recency", "freq", "vol", "time", "target")

training_mat <- data.matrix(training, rownames.force = T)

corr_mat <- cor(training_mat)

corrplot.mixed(corr_mat)

```

Here we can see that frequency of donation and the total blood volume are perfectly positively correlated. We can therefore ignore the total blood volume variable since all of the information will be captured and passed along to our algorithms in the using the variables measured in months and the donation frequency.

```{r remove blood volume}
training <- select(training, -vol)
```


# Cross Validation Folds & Splits

The workflow and code used here is adapted from the *Machine Learning in the Tidyverse* course by Dmitriy Gorenshteyn (DataCamp 2019). The approach involves using list-columns to store list objects (such as model output) in a tibble, which is a special R data frame. These lists can then be iterated over with specialized `map` functions from the `purrr` package to calculate and extract information.

Using the training data, we will create a 5-fold cross-validation split tibble. This will set us up to itertively generate multiple models on our new train/validation subsets.

```{r create vfold}
# using the rsample library
cv_split <- vfold_cv(training, v = 5)
# cv_split  # uncomment the front of this line if you want to preview
```

The `cv_split` object has five rows and two columns. The first column, `splits`, is a list column containing the training and validation data. The second column is a character vector containing the fold id generated by the `vfold_cv()` function. We can iterate over the `splits` column and extract the train and validation columns 
```{r make train & validation}
cv_data <- cv_split %>%
   mutate(train = map(splits, ~training(.x)),
          validate = map(splits, ~testing(.x)))
glimpse(cv_data)
```
We've just created two new list columns containing the data that we can now train and validate models over.

# Model preparation

### linear model

```{r linear model}
cv_models_lm <- cv_data %>%
   mutate(lm_model = map(train, ~lm(formula = target~., data = .x)))
```


### other models
use the linear model and random forest model code chunks as examples for making another object holding the cross-validation data sets and the specified models.



### random forest

```{r random forest model}
# set forest parameter
n_trees <- 500
# Build a random forest model for each fold
cv_models_rf <- cv_data %>% 
  mutate(rf_model = map(train, ~ranger(formula = target~., data = .x,
                                    num.trees = n_trees,)))
```





# Model evaluation

The *cross-validation model evaluation process* follows the same general set of steps iterated over each fold:
1. extract the actual target values from the validation set
2. use the models to make target predictions that will be compared to the actual target
3. for each fold, calculate the Mean Absolute Error (`MAE`) where $$ MAE = \frac{\sum^n_{i=1}|Actual_i - Predicted_i|}{n} $$
4. Take the average over all MAE values to determine which model performs best on these sets of training & validation splits

In the following subsections, the eval code chunk follows a similar pattern: 

+ the *actual* and *predicted* values are extracted from the validation sets into a `cv_prep_`


Lets see how other models and randocompared.

### linear model

```{r lm predict}
# extract actual values
cv_prep_lm <- cv_models_lm %>%
   mutate(validate_actual = map(validate, ~.x$target),
          validate_predicted = map2(.x = lm_model, .y = validate, 
                                    ~predict(.x, .y)))
```


```{r lm eval}
# the function mae() is from library(Metrics)
# Calculate the mean absolute error for each validate fold 
cv_eval_lm <- cv_prep_lm %>% 
  mutate(validate_mae = map2_dbl(.x = validate_actual, 
                                 .y = validate_predicted, 
                                 ~mae(actual = .x, predicted = .y)))

# Print the validate_mae column
cv_eval_lm$validate_mae

```

The average mean absolute error across all linear models was `r mean(cv_eval_lm$validate_mae)`.



### other models
use the linear model and random forest model code chunks as examples for making another object holding the cross-validation data sets and the specified models.

### random forest

```{r rf predict}
# Generate predictions using the random forest model
cv_prep_rf <- cv_models_rf %>% 
  mutate(validate_actual = map(validate, ~.x$target),
         validate_predicted = map2(.x = rf_model, .y = validate, ~predict(.x, .y)$predictions))
```

```{r rf eval}

# Calculate validate MAE for each fold
cv_eval_rf <- cv_prep_rf %>% 
  mutate(validate_mae = map2_dbl(validate_actual, validate_predicted, ~mae(actual = .x, predicted = .y)))

# Print the validate_mae column
cv_eval_rf$validate_mae

# Calculate the mean of validate_mae column
mean(cv_eval_rf$validate_mae)
```

**Tune hyper-parameters**

```{r hyper parameters}

# Prepare for tuning cross validation folds by varying mtry
cv_tune <- cv_data %>% 
   crossing(mtry = 1:3)

# Build a model for each fold & mtry combination
cv_model_tunerf <- cv_tune %>% 
  mutate(rf_model = 
            map2(.x = train, .y = mtry, 
                 ~ranger(formula = target~., data = .x, 
                         mtry = .y, num.trees = n_trees)))

glimpse(cv_model_tunerf) 
```


```{r rf mae}
# Generate validate predictions for each model
cv_prep_tunerf <- cv_model_tunerf %>% 
  mutate(rf_validate_actual = map(validate, ~.x$target),
     rf_validate_predicted = map2(.x = rf_model, .y = validate, ~predict(.x, .y)$predictions))

# Calculate validate MAE for each fold and mtry combination
cv_eval_tunerf <- cv_prep_tunerf %>% 
  mutate(rf_validate_mae = map2_dbl(.x = rf_validate_actual, .y = rf_validate_predicted, ~mae(actual = .x, predicted = .y)))

# Calculate the mean validate_mae for each mtry used  
cv_eval_tunerf %>% 
  group_by(mtry) %>% 
  summarise(rf_mean_mae = mean(rf_validate_mae))

```

Select random forest model with the lowest average mae.

```{r best mtry}
best <- filter(cv_eval_tunerf, rf_validate_mae == min(cv_eval_tunerf$rf_validate_mae))
kable(tibble(id = best$id, mtry = best$mtry))
```

Use these parameters to traing our best rf model.
```{r select best rf model}
# Build the model using all training data and the best performing parameter
best_model <- ranger(formula = target~., data = training,
                     mtry = best$mtry, num.trees = n_trees)
```


# Model Output on Test Data

```{r load test data}
test_data <- read_csv("project test data.csv")
names(test_data) <- c("id", "recency", "freq", "vol", "time")
# remove id
test <- select(test_data,-id, -vol)
```


```{r predict test data vector}
# Predict life_expectancy for the testing_data
test_predicted <- predict(best_model, test)$predictions
rf_test_prediction <- cbind(test_data$id, test_predicted)

```


# Discussion




# References

Data is courtesy of Yeh, I-Cheng via the UCI Machine Learning repository (https://archive.ics.uci.edu/ml/datasets/Blood+Transfusion+Service+Center)

https://archive.ics.uci.edu/ml/machine-learning-databases/blood-transfusion/transfusion.names

Code examples were borrowed from DataCamp course material presented by Dmitriy Gorenshteyn, "Machine Learning in the Tidyverse" https://www.datacamp.com/courses/machine-learning-in-the-tidyverse

# Appendix

```{r random forest predicted test vector, include=F, eval=F}
kable(rf_test_prediction)
```




